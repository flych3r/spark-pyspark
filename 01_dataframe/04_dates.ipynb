{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('dates').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock = spark.read.csv(\n",
    "    '../data/appl_stock.csv',\n",
    "    inferSchema=True, header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stock = df_stock.withColumn('Date', df_stock['Date'].cast('timestamp'))\n",
    "df_stock.printSchema()\n",
    "df_stock.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofyear, dayofmonth, dayofweek, date_format, year, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+---------------+---------------+\n",
      "|               Date|dayofmonth(Date)|dayofyear(Date)|dayofweek(Date)|\n",
      "+-------------------+----------------+---------------+---------------+\n",
      "|2010-01-04 00:00:00|               4|              4|              2|\n",
      "|2010-01-05 00:00:00|               5|              5|              3|\n",
      "|2010-01-06 00:00:00|               6|              6|              4|\n",
      "|2010-01-07 00:00:00|               7|              7|              5|\n",
      "|2010-01-08 00:00:00|               8|              8|              6|\n",
      "|2010-01-11 00:00:00|              11|             11|              2|\n",
      "|2010-01-12 00:00:00|              12|             12|              3|\n",
      "|2010-01-13 00:00:00|              13|             13|              4|\n",
      "|2010-01-14 00:00:00|              14|             14|              5|\n",
      "|2010-01-15 00:00:00|              15|             15|              6|\n",
      "+-------------------+----------------+---------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stock.select(df_stock['Date'], dayofmonth(df_stock['Date']), dayofyear(df_stock['Date']), dayofweek(df_stock['Date'])).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+\n",
      "|Year|High avg|Low avg|\n",
      "+----+--------+-------+\n",
      "|2016|  105.43| 103.69|\n",
      "|2015|  121.24| 118.86|\n",
      "|2014|  297.56| 292.99|\n",
      "|2013|  477.64| 468.25|\n",
      "|2012|  581.83| 569.92|\n",
      "|2011|  367.42| 360.30|\n",
      "|2010|  262.37| 256.85|\n",
      "+----+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "high_low_by_year = df_stock.groupBy(year(df_stock['Date'])).mean('High', 'Low')\n",
    "high_low_by_year = high_low_by_year.select(\n",
    "    high_low_by_year['year(Date)'].alias('Year'),\n",
    "    format_number('avg(High)', 2).alias('High avg'),\n",
    "    format_number('avg(Low)', 2).alias('Low avg')\n",
    ")\n",
    "high_low_by_year = high_low_by_year.orderBy(high_low_by_year['Year'].desc())\n",
    "high_low_by_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}