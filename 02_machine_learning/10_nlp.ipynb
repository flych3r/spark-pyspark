{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = spark.createDataFrame([\n",
    "    (0, 'Hi I heard about Spark'),\n",
    "    (1, 'I wish Java could use case classes'),\n",
    "    (2, 'Logistic,regression,models,are,neat'),\n",
    "    (3, 'I saw the red balloon'),\n",
    "    (4, 'Mary had a little lamb')\n",
    "], ['id', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------+\n",
      "|id |sentence                           |\n",
      "+---+-----------------------------------+\n",
      "|0  |Hi I heard about Spark             |\n",
      "|1  |I wish Java could use case classes |\n",
      "|2  |Logistic,regression,models,are,neat|\n",
      "|3  |I saw the red balloon              |\n",
      "|4  |Mary had a little lamb             |\n",
      "+---+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "regexTokenizer = RegexTokenizer(inputCol='sentence', outputCol='words', pattern='\\\\W')\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = tokenizer.transform(sentence_df)\n",
    "regex_tokenized_df = regexTokenizer.transform(sentence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------+------------------------------------------+------+\n",
      "|id |sentence                           |words                                     |tokens|\n",
      "+---+-----------------------------------+------------------------------------------+------+\n",
      "|0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|1  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|2  |Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "|3  |I saw the red balloon              |[i, saw, the, red, balloon]               |5     |\n",
      "|4  |Mary had a little lamb             |[mary, had, a, little, lamb]              |5     |\n",
      "+---+-----------------------------------+------------------------------------------+------+\n",
      "\n",
      "+---+-----------------------------------+------------------------------------------+------+\n",
      "|id |sentence                           |words                                     |tokens|\n",
      "+---+-----------------------------------+------------------------------------------+------+\n",
      "|0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|1  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|2  |Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "|3  |I saw the red balloon              |[i, saw, the, red, balloon]               |5     |\n",
      "|4  |Mary had a little lamb             |[mary, had, a, little, lamb]              |5     |\n",
      "+---+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.withColumn('tokens', countTokens(col('words'))).show(truncate=False)\n",
    "regex_tokenized_df.withColumn('tokens', countTokens(col('words'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------+------------------------------------------+------------------------------------+\n",
      "|id |sentence                           |words                                     |filtered                            |\n",
      "+---+-----------------------------------+------------------------------------------+------------------------------------+\n",
      "|0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |[hi, heard, spark]                  |\n",
      "|1  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|[wish, java, use, case, classes]    |\n",
      "|2  |Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |[logistic, regression, models, neat]|\n",
      "|3  |I saw the red balloon              |[i, saw, the, red, balloon]               |[saw, red, balloon]                 |\n",
      "|4  |Mary had a little lamb             |[mary, had, a, little, lamb]              |[mary, little, lamb]                |\n",
      "+---+-----------------------------------+------------------------------------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n",
    "remover.transform(regex_tokenized_df).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------+------------------------------------------+------------------------------------------------------------------+\n",
      "|id |sentence                           |words                                     |ngrams                                                            |\n",
      "+---+-----------------------------------+------------------------------------------+------------------------------------------------------------------+\n",
      "|0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |[hi i, i heard, heard about, about spark]                         |\n",
      "|1  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|[i wish, wish java, java could, could use, use case, case classes]|\n",
      "|2  |Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |[logistic regression, regression models, models are, are neat]    |\n",
      "|3  |I saw the red balloon              |[i, saw, the, red, balloon]               |[i saw, saw the, the red, red balloon]                            |\n",
      "|4  |Mary had a little lamb             |[mary, had, a, little, lamb]              |[mary had, had a, a little, little lamb]                          |\n",
      "+---+-----------------------------------+------------------------------------------+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = NGram(n=2, inputCol='words', outputCol='ngrams')\n",
    "\n",
    "ngram.transform(regex_tokenized_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_TF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=20)\n",
    "featurized_df = hashing_TF.transform(regex_tokenized_df)\n",
    "\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idf_model = idf.fit(featurized_df)\n",
    "rescaled_data = idf_model.transform(featurized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|                           sentence|                                    rawFeatures|                                                                                            features|\n",
      "+-----------------------------------+-----------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|             Hi I heard about Spark|             (20,[6,8,13,16],[1.0,1.0,1.0,2.0])|      (20,[6,8,13,16],[0.6931471805599453,1.0986122886681098,0.6931471805599453,0.8109302162163288])|\n",
      "| I wish Java could use case classes|(20,[0,2,7,13,15,16],[1.0,1.0,2.0,1.0,1.0,1.0])|(20,[0,2,7,13,15,16],[0.6931471805599453,1.0986122886681098,1.3862943611198906,0.6931471805599453...|\n",
      "|Logistic,regression,models,are,neat|       (20,[3,4,6,11,19],[1.0,1.0,1.0,1.0,1.0])|(20,[3,4,6,11,19],[0.6931471805599453,1.0986122886681098,0.6931471805599453,0.6931471805599453,1....|\n",
      "|              I saw the red balloon|     (20,[3,12,15,16,17],[1.0,1.0,1.0,1.0,1.0])|(20,[3,12,15,16,17],[0.6931471805599453,1.0986122886681098,0.6931471805599453,0.4054651081081644,...|\n",
      "|             Mary had a little lamb|       (20,[0,7,9,10,11],[1.0,1.0,1.0,1.0,1.0])|(20,[0,7,9,10,11],[0.6931471805599453,0.6931471805599453,1.0986122886681098,1.0986122886681098,0....|\n",
      "+-----------------------------------+-----------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.select('sentence', 'rawFeatures', 'features').show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, 'a b c'.split()),\n",
    "    (1, 'a b b c a'.split())\n",
    "], ['id', 'words'])\n",
    "\n",
    "cv = CountVectorizer(inputCol='words', outputCol='features', vocabSize=3, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_spam = spark.read.csv('../data/SMSSpamCollection.tsv', inferSchema=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n",
      "+-------+----+--------------------+\n",
      "|summary| _c0|                 _c1|\n",
      "+-------+----+--------------------+\n",
      "|  count|5574|                5574|\n",
      "|   mean|null|               645.0|\n",
      "| stddev|null|                null|\n",
      "|    min| ham| &lt;#&gt;  in mc...|\n",
      "|    max|spam|â€¦ we r stayin her...|\n",
      "+-------+----+--------------------+\n",
      "\n",
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms_spam.printSchema()\n",
    "sms_spam.describe().show()\n",
    "sms_spam.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_spam = sms_spam\\\n",
    "    .withColumnRenamed('_c0','class')\\\n",
    "    .withColumnRenamed('_c1','text')\n",
    "sms_spam = sms_spam.withColumn('length', length(sms_spam['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "+-----+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms_spam.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+------------+\n",
      "|class|      avg(length)|count(class)|\n",
      "+-----+-----------------+------------+\n",
      "|  ham|71.45431945307645|        4827|\n",
      "| spam|138.6706827309237|         747|\n",
      "+-----+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms_spam.groupBy('class').agg({'length': 'mean', 'class': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer, VectorAssembler\n",
    "\n",
    "ham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='tokens')\n",
    "stopremove = StopWordsRemover(inputCol='tokens',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='count_vec')\n",
    "idf = IDF(inputCol='count_vec', outputCol='tf_idf')\n",
    "assembler = VectorAssembler(inputCols=['tf_idf', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppln = Pipeline(stages=[\n",
    "    tokenizer, stopremove, count_vec, idf, assembler, clf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+-----+\n",
      "|class|                text|length|label|\n",
      "+-----+--------------------+------+-----+\n",
      "|  ham|Go until jurong p...|   111|  0.0|\n",
      "|  ham|Ok lar... Joking ...|    29|  0.0|\n",
      "| spam|Free entry in 2 a...|   155|  1.0|\n",
      "|  ham|U dun say so earl...|    49|  0.0|\n",
      "|  ham|Nah I don't think...|    61|  0.0|\n",
      "+-----+--------------------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms_spam = ham_spam_to_num.fit(sms_spam).transform(sms_spam)\n",
    "sms_spam.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sms, test_sms = sms_spam.randomSplit([.7, .8], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_model = ppln.fit(train_sms)\n",
    "\n",
    "predictions = spam_model.transform(test_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|class|                text|length|label|              tokens|         stop_tokens|           count_vec|              tf_idf|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  ham| &lt;#&gt;  in mc...|    36|  0.0|[, &lt;#&gt;, , i...|[, &lt;#&gt;, , m...|(8490,[4,13,6183]...|(8490,[4,13,6183]...|(8491,[4,13,6183,...|[-156.87817715538...|[1.0,1.0617857285...|       0.0|\n",
      "|  ham| &lt;#&gt;  mins ...|    51|  0.0|[, &lt;#&gt;, , m...|[, &lt;#&gt;, , m...|(8490,[4,13,41,23...|(8490,[4,13,41,23...|(8491,[4,13,41,23...|[-308.06882545624...|[1.0,4.4088653034...|       0.0|\n",
      "|  ham| &lt;DECIMAL&gt; ...|   132|  0.0|[, &lt;decimal&gt...|[, &lt;decimal&gt...|(8490,[4,87,126,2...|(8490,[4,87,126,2...|(8491,[4,87,126,2...|[-600.98775421349...|[1.0,6.9693889631...|       0.0|\n",
      "|  ham| came to look at ...|   103|  0.0|[, came, to, look...|[, came, look, fl...|(8490,[4,9,167,21...|(8490,[4,9,167,21...|(8491,[4,9,167,21...|[-588.10304572326...|[1.0,1.1754513270...|       0.0|\n",
      "|  ham| gonna let me kno...|    95|  0.0|[, gonna, let, me...|[, gonna, let, kn...|(8490,[4,10,79,85...|(8490,[4,10,79,85...|(8491,[4,10,79,85...|[-558.62863323533...|[1.0,8.0907629467...|       0.0|\n",
      "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9744811160258592\n",
      "weightedPrecision: 0.974842530521143\n",
      "weightedRecall: 0.9744811160258591\n",
      "weightedFMeasure: 0.9746342366567277\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction')\n",
    "\n",
    "for metric in ['accuracy', 'weightedPrecision', 'weightedRecall', 'weightedFMeasure']:\n",
    "    m = evaluator.setMetricName(metric).evaluate(predictions)\n",
    "    print(f'{metric}: {m}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}